{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushithakondreddy/haihua_INFO5731_Spring2020/blob/main/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "vR0L3_CreM_A",
        "outputId": "ce408f80-b77e-47ee-913f-65fee3668e53"
      },
      "source": [
        "with open('/content/01-05-1 Adams v Tanner.txt') as file:\n",
        "    text = file.read()\n",
        "\n",
        "number_of_sentences = lambda x : len(str(x).split('. '))\n",
        "print(\"Number of sentences:\", number_of_sentences(text))\n",
        "\n",
        "number_of_words = lambda x : len(str(x).split(' '))\n",
        "print('Number of words:', number_of_words(text))\n",
        "\n",
        "characters_wihtout_space = [char for char in list(text) if char != ' ']\n",
        "print('Number of characters (without space):', len(characters_wihtout_space))\n",
        "print('Number of charcters: ', len(list(text)))\n",
        "\n",
        "def average_word_length(text):\n",
        "    total = 0\n",
        "    for word in text.split(' '):\n",
        "        total += len(word)\n",
        "    return total/number_of_words(text)\n",
        "\n",
        "average_word_length(text)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "print('Number of stopwords:', len([word for word in text.split() if word in stopwords.words('english')]))\n",
        "\n",
        "special_characters = '!@#$^&*'\n",
        "total = 0\n",
        "for s_char in special_characters:\n",
        "    count = len([word for word in text.split(' ') if word.startswith(s_char)])\n",
        "    total += count\n",
        "print('Number of special characters:', total)\n",
        "\n",
        "number_of_numerics = len([x for x in list(text) if x.isnumeric()])\n",
        "print('Number of numerics:', number_of_numerics)\n",
        "\n",
        "number_of_upper = len([word for word in text.split() if word.isupper()])\n",
        "print('Number of uppercase words:', number_of_upper)\n",
        "\n",
        "# Turn all text into lowercase, Remove Punctuations, \n",
        "text = \" \".join(word.lower() for word in text.split())\n",
        "punctuations = r\",:;'()[]{}!?-*—&/\\><§*&%$#@+=`~\"\n",
        "chars = [char for char in list(text) if char not in punctuations]\n",
        "text = ''.join(chars)\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "words = [word for word in text.split() if word not in stopwords.words('english')]\n",
        "text = ' '.join(words)\n",
        "from collections import Counter\n",
        "from numpy import argsort, array\n",
        "words = [word for word in text.split(' ')]\n",
        "counts = Counter(words)\n",
        "idx = argsort(array(list(counts.values())))[-10:]\n",
        "freq = array(list(counts.keys()))[idx]\n",
        "not_freq = [word for word in text.split() if word not in freq]\n",
        "print('Frequent words:', freq)\n",
        "print()\n",
        "text = ' '.join(not_freq)\n",
        "from collections import Counter\n",
        "from numpy import argsort, array\n",
        "words = [word for word in text.split(' ')]\n",
        "counts = Counter(words)\n",
        "idx = argsort(array(list(counts.values())))[:10]\n",
        "rare = array(list(counts.keys()))[idx]\n",
        "not_rare = [word for word in text.split() if word not in freq]\n",
        "print('Rare words:', rare)\n",
        "print()\n",
        "text = ' '.join(not_rare)\n",
        "from textblob import TextBlob\n",
        "corrected = []\n",
        "for word in text.split(' '):\n",
        "    word = str(TextBlob(word).correct())\n",
        "    corrected.append(word)\n",
        "text = ' '.join(corrected)\n",
        "tokens = str(TextBlob(text).words)\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "stems = [st.stem(word) for word in text.split()]\n",
        "text = ' '.join(stems)\n",
        "\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemmas = [Word(word).lemmatize() for word in text.split()]\n",
        "text = ' '.join(lemmas)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'Sentences' :text.split('. ')})\n",
        "df.to_csv('clean_text.csv')\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 204\n",
            "Number of words: 3576\n",
            "Number of characters (without space): 16878\n",
            "Number of charcters:  20453\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Number of stopwords: 1679\n",
            "Number of special characters: 18\n",
            "Number of numerics: 356\n",
            "Number of uppercase words: 84\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Frequent words: ['growing' 'claimants' 'case' 'court' 'right' 'v.' 'levy' 'lien' 'crop'\n",
            " 'execution']\n",
            "\n",
            "Rare words: ['assumed' 'indemnity' 'confirmation' 'subsequently' 'acquiesced'\n",
            " 'inference' 'mean' 'valuethe' 'require' 'approved']\n",
            "\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>740 suprem alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>adam manner norton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>june term 1843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sycosi writ error circuit sumter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>agre “to render yield pay a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>one half wheat rye corn grain rais farm year t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>2 file file situation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>neg treatment neg treatment result situation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>histori histori result situation.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>247 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Sentences\n",
              "0                                                5 all\n",
              "1                                   740 suprem alabama\n",
              "2                                   adam manner norton\n",
              "3                                       june term 1843\n",
              "4                     sycosi writ error circuit sumter\n",
              "..                                                 ...\n",
              "242                        agre “to render yield pay a\n",
              "243  one half wheat rye corn grain rais farm year t...\n",
              "244                              2 file file situation\n",
              "245       neg treatment neg treatment result situation\n",
              "246                  histori histori result situation.\n",
              "\n",
              "[247 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKSHL3Ya8nUp",
        "outputId": "ed9db995-1324-4a86-8285-ed9c91629e64"
      },
      "source": [
        "import numpy as np\r\n",
        "tf1 = (df['Sentences'].apply(lambda x: pd.value_counts(str(x).split(\" \"))).sum(axis = 0).reset_index())\r\n",
        "tf1.columns = ['words','tf']\r\n",
        "# tf1.tf = tf1.tf/df.shape[0] # ratio of occurence diveded by the total number of sentences\r\n",
        "tf1.tf = np.where(tf1.tf != 0, 1+np.log(tf1.tf), 0)\r\n",
        "tf1\r\n",
        "\r\n",
        "\r\n",
        "n1_gram = [str(n1) for n1 in (TextBlob(text).ngrams(1))]\r\n",
        "counts = Counter(n1_gram)\r\n",
        "idx = argsort(array(list(counts.values())))[-10:]\r\n",
        "top10_n1_gram = array(list(counts.keys()))[idx]\r\n",
        "print(\"top 10 n1 gram\\n\\n\", top10_n1_gram)\r\n",
        "\r\n",
        "n2_gram = [str(n2) for n2 in (TextBlob(text).ngrams(2))]\r\n",
        "counts = Counter(n2_gram)\r\n",
        "idx = argsort(array(list(counts.values())))[-10:]\r\n",
        "top10_n2_gram = array(list(counts.keys()))[idx]\r\n",
        "print(\"\\n\\ntop 10 n2 gram\\n\\n\", top10_n2_gram)\r\n",
        "\r\n",
        "n3_gram = [str(n3) for n3 in (TextBlob(text).ngrams(3))]\r\n",
        "counts = Counter(n3_gram)\r\n",
        "idx = argsort(array(list(counts.values())))[-10:]\r\n",
        "top10_n3_gram = array(list(counts.keys()))[idx]\r\n",
        "print(\"\\n\\ntop 10 n3 gram\\n\\n\", top10_n3_gram)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 10 n1 gram\n",
            "\n",
            " [\"['’']\" \"['sale']\" \"['posse']\" \"['cotton']\" \"['gather']\" \"['plaintiff']\"\n",
            " \"['contract']\" \"['all']\" \"['rep']\" \"['law']\"]\n",
            "\n",
            "\n",
            "top 10 n2 gram\n",
            "\n",
            " [\"['levi', 'sold']\" \"['2', 'john']\" \"['good', 'faith']\"\n",
            " \"['appeal', 'circuit']\" \"['can', 'not']\" \"['while', 'foot']\"\n",
            " \"['fieri', 'facial']\" \"['tri', 'hon']\" \"['john', 'rep']\"\n",
            " \"['common', 'law']\"]\n",
            "\n",
            "\n",
            "top 10 n3 gram\n",
            "\n",
            " [\"['appeal', 'citi', 'montgomery']\" \"['case', 'cite', 'headnot']\"\n",
            " \"['citi', 'montgomery', 'tri']\" \"['montgomery', 'tri', 'hon']\"\n",
            " \"['tri', 'hon', 'john']\" \"['immedi', 'futur', 'time']\"\n",
            " \"['sheriff', 'offic', 'writ']\" \"['5', 'all', '740']\"\n",
            " \"['2', 'john', 'rep']\" \"['all', 'grover', 'convers']\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSv6fVhOfFmv",
        "outputId": "b72150ac-a023-4eb1-adf4-1034489769dd"
      },
      "source": [
        "# Write your code here\n",
        "def rem_lead_zero(o_ip):  \n",
        "    n_ip = \".\".join([str (int(i)) for i in o_ip.split(\".\")])   \n",
        "    return n_ip \n",
        "o_ip = \"260.08.094.109\"\n",
        "print(rem_lead_zero(o_ip))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xdJpDx9gjbX",
        "outputId": "c8b2f5bf-ddfd-49b3-9cb8-edb5183f81d3"
      },
      "source": [
        "# Write your code here\r\n",
        "import re\r\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\r\n",
        "year = re.findall('(\\d{4})', sentence)\r\n",
        "year"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2010', '1000', '2010', '2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}