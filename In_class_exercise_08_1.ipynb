{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In_class_exercise_08-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushithakondreddy/haihua_INFO5731_Spring2020/blob/main/In_class_exercise_08_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8znB8PUhKf6u"
      },
      "source": [
        "# **The eighth in-class-exercise (20 points in total, 3/30/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlha0I6SKf7B"
      },
      "source": [
        "The data for this exercise is from the dataset you created from assignment three. Please perform answer the following questions based on your data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnao0CRFKf7D"
      },
      "source": [
        "## (1) (10 points) Write a python program to extract the sentiment related terms from the corpus. You may use python package such as polyglot or external lexicon resources in the question. Rank the sentiment related terms by frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u86ziHn_Kf7E",
        "outputId": "e06970d7-6ee3-426c-a958-63c501023806"
      },
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "from textblob import TextBlob\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vJuBGiF6KiB2",
        "outputId": "f827cdfa-9f8f-47de-d83c-a8f8fc1c766f"
      },
      "source": [
        "dd = pd.read_csv(\"/content/review_data.csv\")\n",
        "dd = dd[['review_content']]\n",
        "dd['review_content'] = dd['review_content'].str.replace('[^\\w\\s]','')\n",
        "dd['review_content'] = dd['review_content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "dd['review_content'] = dd['review_content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "dd['review_content'] = dd['review_content'].apply(lambda x: nltk.word_tokenize(x))\n",
        "values = (dd['review_content']).apply(lambda x: pd.value_counts(x)).sum(axis = 0).reset_index()\n",
        "values.columns = ['Words_List', 'tf']\n",
        "values['polarity_Value'] = values['Words_List'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "terms = values.loc[values['polarity_Value'] != 0].sort_values(by='tf', ascending=False)\n",
        "terms = terms.reset_index(drop=True)\n",
        "terms.head(50)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words_List</th>\n",
              "      <th>tf</th>\n",
              "      <th>polarity_Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>good</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dark</td>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>best</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>many</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>much</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>really</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>great</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bad</td>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>comic</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>real</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>mental</td>\n",
              "      <td>21.0</td>\n",
              "      <td>-0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>better</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>far</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>little</td>\n",
              "      <td>16.0</td>\n",
              "      <td>-0.187500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>amazing</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>first</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>action</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>boring</td>\n",
              "      <td>13.0</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>crazy</td>\n",
              "      <td>13.0</td>\n",
              "      <td>-0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>high</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>long</td>\n",
              "      <td>12.0</td>\n",
              "      <td>-0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>whole</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>laugh</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>perfect</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>brilliant</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ill</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>slow</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>mentally</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>mean</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-0.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>absolutely</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>right</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>interesting</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>poor</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>completely</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>violent</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>realistic</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>true</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>hard</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-0.291667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>least</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>love</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>main</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>wrong</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>important</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>kind</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>excellent</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>win</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>new</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>poorly</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>sure</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>less</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-0.166667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Words_List    tf  polarity_Value\n",
              "0          good  55.0        0.700000\n",
              "1          dark  38.0       -0.150000\n",
              "2          best  35.0        1.000000\n",
              "3          many  33.0        0.500000\n",
              "4          much  32.0        0.200000\n",
              "5        really  29.0        0.200000\n",
              "6         great  25.0        0.800000\n",
              "7           bad  24.0       -0.700000\n",
              "8         comic  24.0        0.250000\n",
              "9          real  22.0        0.200000\n",
              "10       mental  21.0       -0.100000\n",
              "11       better  20.0        0.500000\n",
              "12          far  19.0        0.100000\n",
              "13       little  16.0       -0.187500\n",
              "14      amazing  15.0        0.600000\n",
              "15        first  14.0        0.250000\n",
              "16       action  14.0        0.100000\n",
              "17       boring  13.0       -1.000000\n",
              "18        crazy  13.0       -0.600000\n",
              "19         high  13.0        0.160000\n",
              "20         long  12.0       -0.050000\n",
              "21        whole  12.0        0.200000\n",
              "22        laugh  11.0        0.300000\n",
              "23      perfect  11.0        1.000000\n",
              "24    brilliant  10.0        0.900000\n",
              "25          ill  10.0       -0.500000\n",
              "26         slow  10.0       -0.300000\n",
              "27     mentally  10.0       -0.100000\n",
              "28         mean  10.0       -0.312500\n",
              "29   absolutely  10.0        0.200000\n",
              "30        right  10.0        0.285714\n",
              "31  interesting   9.0        0.500000\n",
              "32         poor   9.0       -0.400000\n",
              "33   completely   9.0        0.100000\n",
              "34      violent   8.0       -0.800000\n",
              "35    realistic   8.0        0.166667\n",
              "36         true   8.0        0.350000\n",
              "37         hard   8.0       -0.291667\n",
              "38        least   8.0       -0.300000\n",
              "39         love   8.0        0.500000\n",
              "40         main   8.0        0.166667\n",
              "41        wrong   8.0       -0.500000\n",
              "42    important   7.0        0.400000\n",
              "43         kind   7.0        0.600000\n",
              "44    excellent   7.0        1.000000\n",
              "45          win   7.0        0.800000\n",
              "46          new   7.0        0.136364\n",
              "47       poorly   7.0       -0.400000\n",
              "48         sure   7.0        0.500000\n",
              "49         less   7.0       -0.166667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjL4My8Kf7G"
      },
      "source": [
        "## (2) (10 points) Compare the performance of the following tools in sentiment identification: TextBlob (https://textblob.readthedocs.io/en/dev/), VADER (https://github.com/cjhutto/vaderSentiment), TFIDF-based Support Vector Machine (SVM) (Split your data into training and testing data). Take your own annotation as the standard answers. \n",
        "\n",
        "Reference code: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C70COLXzRg2S",
        "outputId": "22095459-4cc9-43c2-e159-cb98b0f4a6db"
      },
      "source": [
        "original_positive = 0\n",
        "original_negative = 0\n",
        "original_neutral = 0\n",
        "for i in dd['Sentiment']:\n",
        "  if i == 'Positive':\n",
        "    original_positive += 1\n",
        "  elif i == 'Negative':\n",
        "    original_negative += 1\n",
        "  elif i == 'Neutral':\n",
        "    original_neutral += 1\n",
        "print('Original Positive = ', original_positive)\n",
        "print('Original Negative = ', original_negative)\n",
        "print('Original Neutral = ', original_neutral)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Positive =  47\n",
            "Original Negative =  42\n",
            "Original Neutral =  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbSAz1xHMIpc",
        "outputId": "09b0274e-d37a-4cba-d1bc-b1f6e9be7470"
      },
      "source": [
        "!pip install polyglot"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52557 sha256=4af3056666376a58e9a7ea656a1b3a8702874aa8d82120af2fbcdd297bf9aab1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlkeRpJeMRJP",
        "outputId": "1b0a55eb-bf30-4b76-bcb3-67641e586f50"
      },
      "source": [
        "!pip install pyicu\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyicu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/46/fa08c8efae2951e67681ec24319f789fc1a74e2096dd74373e34c79319de/PyICU-2.6.tar.gz (233kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 112kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 122kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 133kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 143kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 153kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 163kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 174kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 184kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 194kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 204kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 215kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 225kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 5.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyicu\n",
            "  Building wheel for pyicu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyicu: filename=PyICU-2.6-cp37-cp37m-linux_x86_64.whl size=1306439 sha256=9192d8a5dfc17744e4fcfbd0aad90b8592c698811e68fd050cc6513d59b7bf43\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/21/2f/1c91831e8a93537ab21f6b4b935781b681104635fdb0315791\n",
            "Successfully built pyicu\n",
            "Installing collected packages: pyicu\n",
            "Successfully installed pyicu-2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL9jpWh8MT5d",
        "outputId": "3dafc71e-52be-4b2b-d982-c25682a90184"
      },
      "source": [
        "!pip install pycld2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycld2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 104kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycld2\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834283 sha256=157b7abea2f813ec3be62c3f429d5da94290f45c8126d92aed5ffab2a9871257\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "Successfully built pycld2\n",
            "Installing collected packages: pycld2\n",
            "Successfully installed pycld2-0.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S7csHg3MWYC",
        "outputId": "d66d81c9-28f2-4125-cefd-a00bdfd6917e"
      },
      "source": [
        "!pip install morfessor"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: morfessor\n",
            "Successfully installed morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYbqUs2gMZdQ",
        "outputId": "4291fe88-286d-4c60-b669-50c8fb7df029"
      },
      "source": [
        "%%bash\n",
        "polyglot download sentiment2.en"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[polyglot_data] Downloading package sentiment2.en to\n",
            "[polyglot_data]     /root/polyglot_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo0LddofMb7A",
        "outputId": "5dca2161-84c4-44c9-c9c6-ad31a1dc6a44"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5gvZm-CPTyL",
        "outputId": "b456521e-313e-4984-fc8b-4efaa2163cdf"
      },
      "source": [
        "dd = pd.read_csv(\"/content/review_data.csv\")\n",
        "original_positive = 0\n",
        "original_negative = 0\n",
        "original_neutral = 0\n",
        "for i in dd['Sentiment']:\n",
        "  if i == 'Positive':\n",
        "    original_positive += 1\n",
        "  elif i == 'Negative':\n",
        "    original_negative += 1\n",
        "  elif i == 'Neutral':\n",
        "    original_neutral += 1\n",
        "print('Original Positive = ', original_positive)\n",
        "print('Original Negative = ', original_negative)\n",
        "print('Original Neutral = ', original_neutral)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Positive =  47\n",
            "Original Negative =  42\n",
            "Original Neutral =  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "s6HE5hdnQm7g",
        "outputId": "97153bd1-66b5-4f20-80c7-af9fab952988"
      },
      "source": [
        "import re\n",
        "dd['Removal of special characters'] = dd['review_content'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "\n",
        "#Removal of punctuations :\n",
        "\n",
        "dd['Punctuation'] = dd['Removal of special characters'].str.replace('[^\\w\\s]','')\n",
        "\n",
        "#Removal of numbers:\n",
        "\n",
        "dd['Removal of numbers'] = dd['Punctuation'].str.replace('\\d+', '')\n",
        "\n",
        "#Lower case all texts:\n",
        "\n",
        "dd['Lower Case words'] = dd['Removal of numbers'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "#Removing stopwords by using stopwordslist\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "dd['removed stopwords'] = dd['Lower Case words'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "dd"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review_content</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Removal of special characters</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Lower Case words</th>\n",
              "      <th>removed stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>i was a person that saw all the hype and claim...</td>\n",
              "      <td>person saw hype claims masterpiece overreactin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Every once in a while a movie comes  that trul...</td>\n",
              "      <td>Every once in a while a movie comes  that trul...</td>\n",
              "      <td>Every once in a while a movie comes  that trul...</td>\n",
              "      <td>every once in a while a movie comes that truly...</td>\n",
              "      <td>every movie comes truly makes impact joaquin p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>movie felt alone isolated truly relate underst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Truly a masterpiece  The Best Hollywood film o...</td>\n",
              "      <td>Truly a masterpiece  The Best Hollywood film o...</td>\n",
              "      <td>Truly a masterpiece  The Best Hollywood film o...</td>\n",
              "      <td>truly a masterpiece the best hollywood film of...</td>\n",
              "      <td>truly masterpiece best hollywood film one best...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>time movies anticipated like end falling short...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>Dark, depressing and unsettling film with a ha...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Dark  depressing and unsettling film with a ha...</td>\n",
              "      <td>Dark  depressing and unsettling film with a ha...</td>\n",
              "      <td>Dark  depressing and unsettling film with a ha...</td>\n",
              "      <td>dark depressing and unsettling film with a hau...</td>\n",
              "      <td>dark depressing unsettling film haunting score...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>I just didn't get the hype about this movie. A...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>I just didn t get the hype about this movie  A...</td>\n",
              "      <td>I just didn t get the hype about this movie  A...</td>\n",
              "      <td>I just didn t get the hype about this movie  A...</td>\n",
              "      <td>i just didn t get the hype about this movie an...</td>\n",
              "      <td>get hype movie entertaining violent movies nea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>Story was just really unconvincing. Nobody rea...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Story was just really unconvincing  Nobody rea...</td>\n",
              "      <td>Story was just really unconvincing  Nobody rea...</td>\n",
              "      <td>Story was just really unconvincing  Nobody rea...</td>\n",
              "      <td>story was just really unconvincing nobody real...</td>\n",
              "      <td>story really unconvincing nobody really cares ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>JOKER is a gift to the audiences. I felt as a ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>JOKER is a gift to the audiences  I felt as a ...</td>\n",
              "      <td>JOKER is a gift to the audiences  I felt as a ...</td>\n",
              "      <td>JOKER is a gift to the audiences  I felt as a ...</td>\n",
              "      <td>joker is a gift to the audiences i felt as a p...</td>\n",
              "      <td>joker gift audiences felt privileged person si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>There is doing an homage and then there is bor...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>There is doing an homage and then there is bor...</td>\n",
              "      <td>There is doing an homage and then there is bor...</td>\n",
              "      <td>There is doing an homage and then there is bor...</td>\n",
              "      <td>there is doing an homage and then there is bor...</td>\n",
              "      <td>homage borrowing wholesale material film littl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0  ...                                  removed stopwords\n",
              "0            0  ...  person saw hype claims masterpiece overreactin...\n",
              "1            1  ...  every movie comes truly makes impact joaquin p...\n",
              "2            2  ...  movie felt alone isolated truly relate underst...\n",
              "3            3  ...  truly masterpiece best hollywood film one best...\n",
              "4            4  ...  time movies anticipated like end falling short...\n",
              "..         ...  ...                                                ...\n",
              "95          95  ...  dark depressing unsettling film haunting score...\n",
              "96          96  ...  get hype movie entertaining violent movies nea...\n",
              "97          97  ...  story really unconvincing nobody really cares ...\n",
              "98          98  ...  joker gift audiences felt privileged person si...\n",
              "99          99  ...  homage borrowing wholesale material film littl...\n",
              "\n",
              "[100 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm8IlSuRQBuF",
        "outputId": "fdf34e1d-90bd-4424-be9f-85f5b6b25abf"
      },
      "source": [
        "dd['removed stopwords'] = dd['Lower Case words'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "positive = 0\n",
        "negative = 0\n",
        "neutral = 0\n",
        "sentimental_analyzer = SentimentIntensityAnalyzer()\n",
        "for i in dd['removed stopwords']:\n",
        "  sent = TextBlob(i)\n",
        "  if sentimental_analyzer.polarity_scores(i)['compound'] > 0:\n",
        "    positive += 1\n",
        "  elif sentimental_analyzer.polarity_scores(i)['compound'] < 0:\n",
        "    negative += 1\n",
        "  elif sentimental_analyzer.polarity_scores(i)['compound'] == 0:\n",
        "    neutral += 1\n",
        "print(\"Positive = \", positive)\n",
        "print(\"Negative = \", negative)\n",
        "print(\"Neutral = \", neutral)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Positive =  73\n",
            "Negative =  26\n",
            "Neutral =  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz9i2UHMQ-pq",
        "outputId": "c5d91a88-0c92-4893-f620-2e509bb5302d"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.8,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True)\n",
        "train_x_data, test_x_data, train_y_data, test_y_data = train_test_split(dd['removed stopwords'],dd['Sentiment'],test_size=0.33)\n",
        "train = vectorizer.fit_transform(train_x_data)\n",
        "test = vectorizer.transform(test_x_data)\n",
        "linear_class = svm.SVC(kernel='linear')\n",
        "linear_class.fit(train, train_y_data)\n",
        "linear_pred = linear_class.predict(test)\n",
        "report = classification_report(test_y_data, linear_pred, output_dict=True)\n",
        "print('positive: ', report['Positive'])\n",
        "print('negative: ', report['Negative'])\n",
        "print(\"neutral:\", report['Neutral'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive:  {'precision': 0.6111111111111112, 'recall': 0.7333333333333333, 'f1-score': 0.6666666666666666, 'support': 15}\n",
            "negative:  {'precision': 0.6666666666666666, 'recall': 0.7142857142857143, 'f1-score': 0.689655172413793, 'support': 14}\n",
            "neutral: {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Ou28PdRMsR"
      },
      "source": [
        "\"\"\"\n",
        "Above we have :\n",
        "\n",
        "Original Positive =  47\n",
        "Original Negative =  42\n",
        "Original Neutral =  11\n",
        "\n",
        "From above all the models we can see that :\n",
        "By using TextBlob : In this we have more number of positive values and total count with original values are not more accurate to the TextBlob.\n",
        " So, TextBlob is not that accurate to Original values.\n",
        "\n",
        "Vader: By using this model we have got the values of positive : 73 , negative : 26 and neutral values of 1. In this we can see that by using Vader model all the values are \n",
        "near by accurate to the Original values. So, Vader is accurate.\n",
        "\n",
        "SVM : By using this model we have got precision of 0.61 and f1-score of 0.666 which has accurate positive values.\n",
        "\n",
        "So, by comparing all the models Vader is more accurate to that of both SVM and TextBlob."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}